{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from scipy.stats import skew\n",
    "import shutil\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uci_datasets(dataset_ids: list, directory_path: str):\n",
    "    \"\"\"\n",
    "    Pobiera zestawy danych UCI, zapisuje dane w formacie CSV oraz tworzy metadane w formacie JSON.\n",
    "\n",
    "    Funkcja wykonuje następujące kroki dla każdego datasetu:\n",
    "      1. Tworzy katalog docelowy, jeśli nie istnieje.\n",
    "      2. Pobiera dane (cechy oraz zmienną target) za pomocą funkcji fetch_ucirepo.\n",
    "      3. Wybiera pierwszą numeryczną kolumnę target, opierając się na metadanych datasetu.\n",
    "      4. Łączy cechy i wybraną zmienną target w jeden DataFrame.\n",
    "      5. Zapisuje DataFrame jako plik CSV.\n",
    "      6. Generuje metadane, zawierające informacje o datasetcie takie jak liczba cech, statystyki targetu, korelacje cech z targetem oraz procent brakujących wartości.\n",
    "      7. Zapisuje metadane w formacie JSON.\n",
    "\n",
    "    Dzięki temu funkcja umożliwia wygodne pobieranie oraz przygotowywanie datasetów z repozytorium UCI, gotowych do dalszej analizy i eksperymentów.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "        X = dataset.data.features\n",
    "        y = dataset.data.targets\n",
    "\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_frame(name=y.name or 'target')\n",
    "\n",
    "        # Extract numeric target from metadata\n",
    "        variables_metadata = dataset.variables\n",
    "\n",
    "        numeric_target_cols = variables_metadata[\n",
    "            (variables_metadata['role'] == 'Target') &\n",
    "            (variables_metadata['type'].isin(['Integer', 'Numeric', 'Real']))\n",
    "        ]['name'].tolist()\n",
    "\n",
    "        # Choose the numeric target column explicitly defined in metadata\n",
    "        if numeric_target_cols:\n",
    "            target_col = numeric_target_cols[0]\n",
    "        elif isinstance(y, pd.DataFrame) and y.shape[1] > 1:\n",
    "            numeric_targets = y.select_dtypes(include=[np.number]).columns\n",
    "            target_col = numeric_targets[0] if not numeric_targets.empty else y.columns[0]\n",
    "        else:\n",
    "            target_col = y.columns[0] if isinstance(y, pd.DataFrame) else 'target'\n",
    "\n",
    "        y = y[[target_col]]\n",
    "\n",
    "        # Combine X and y\n",
    "        df = pd.concat([X, y], axis=1)\n",
    "\n",
    "        dataset_name = dataset.metadata.name.replace(\" \", \"_\")\n",
    "        filename_base = f\"{dataset_name}_{dataset_id}\"\n",
    "        csv_path = os.path.join(directory_path, f\"{filename_base}.csv\")\n",
    "        meta_path = os.path.join(directory_path, f\"{filename_base}.meta.json\")\n",
    "\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        feature_cols = X.columns.tolist()\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = [col for col in feature_cols if col not in numeric_features]\n",
    "\n",
    "        n_rows, n_cols = df.shape\n",
    "        null_pct = df.isnull().sum().sum() / df.size * 100\n",
    "\n",
    "        target_series = y[target_col].dropna()\n",
    "        if pd.api.types.is_numeric_dtype(target_series):\n",
    "            target_mean = target_series.mean()\n",
    "            target_median = target_series.median()\n",
    "            target_min = target_series.min()\n",
    "            target_max = target_series.max()\n",
    "            target_std = target_series.std()\n",
    "            target_skew = skew(target_series)\n",
    "            target_cv = target_std / (target_mean + 1e-8)\n",
    "            outliers = ((target_series - target_mean).abs() > 3 * target_std).sum()\n",
    "            outlier_ratio = outliers / len(target_series) * 100\n",
    "            q1 = np.percentile(target_series, 25)\n",
    "            q3 = np.percentile(target_series, 75)\n",
    "            iqr = round(q3 - q1, 3)\n",
    "            unique_vals = target_series.nunique()\n",
    "        else:\n",
    "            target_mean = target_median = target_min = target_max = None\n",
    "            target_std = target_skew = target_cv = None\n",
    "            outlier_ratio = iqr = unique_vals = None\n",
    "\n",
    "        corr_with_target = {}\n",
    "        if target_std is not None:\n",
    "            for col in numeric_features:\n",
    "                try:\n",
    "                    corr = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "                    corr_with_target[col] = round(corr, 3)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        metadata = {\n",
    "            \"dataset_name\": dataset.metadata.name,\n",
    "            \"id\": dataset_id,\n",
    "            \"target\": target_col,\n",
    "            \"num_observations\": n_rows,\n",
    "            \"num_features\": len(feature_cols),\n",
    "            \"numeric_features\": numeric_features,\n",
    "            \"categorical_features\": categorical_features,\n",
    "            \"%_null_values\": round(null_pct, 2),\n",
    "            \"target_mean\": round(target_mean, 3) if target_mean is not None else None,\n",
    "            \"target_median\": round(target_median, 3) if target_median is not None else None,\n",
    "            \"target_min\": target_min,\n",
    "            \"target_max\": target_max,\n",
    "            \"target_std_dev\": round(target_std, 3) if target_std is not None else None,\n",
    "            \"target_skewness\": round(target_skew, 3) if target_skew is not None else None,\n",
    "            \"target_coefficient_of_variation\": round(target_cv, 3) if target_cv is not None else None,\n",
    "            \"target_outlier_%\": round(outlier_ratio, 2) if outlier_ratio is not None else None,\n",
    "            \"target_IQR\": iqr,\n",
    "            \"target_unique_values\": unique_vals,\n",
    "            \"feature_target_correlations\": corr_with_target,\n",
    "            \"source\": \"UCI\"\n",
    "        }\n",
    "\n",
    "        def convert(o):\n",
    "            if isinstance(o, (np.generic, np.ndarray)):\n",
    "                return o.item() if hasattr(o, 'item') else o.tolist()\n",
    "            return o\n",
    "\n",
    "        metadata = {k: convert(v) for k, v in metadata.items()}\n",
    "\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        print(f\"✔ Zapisano {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fetch_ucirepo(id=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_summary_table(input_datasets_directory: str, output_table_directory: str):\n",
    "    \"\"\"\n",
    "    Funkcja generuje tabelę zbiorczą podsumowującą metadane datasetów, których pliki metadanych (.meta.json)\n",
    "    znajdują się w katalogu input_datasets_directory.\n",
    "\n",
    "    Działanie funkcji:\n",
    "      1. Tworzy katalog wyjściowy (output_table_directory), jeśli nie istnieje.\n",
    "      2. Iteruje przez wszystkie pliki .meta.json w katalogu wejściowym.\n",
    "      3. Dla każdego pliku metadanych:\n",
    "         - Wczytuje metadane zapisane w formacie JSON.\n",
    "         - Wyodrębnia istotne informacje, takie jak nazwa datasetu, źródło, liczba obserwacji, liczba cech,\n",
    "           liczba cech numerycznych i kategorycznych, procent brakujących wartości oraz statystyki zmiennej target.\n",
    "         - Dla cech numerycznych, które korelują ze zmienną target, określa tę o największej bezwzględnej wartości\n",
    "           korelacji (tzw. Top Correlated Feature).\n",
    "      4. Łączy zebrane informacje w jeden DataFrame.\n",
    "      5. Sortuje tabelę według liczby obserwacji oraz liczby cech.\n",
    "      6. Zapisuje finalną tabelę podsumowującą w formacie CSV w katalogu output_table_directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_table_directory, exist_ok=True)\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for file in os.listdir(input_datasets_directory):\n",
    "        if file.endswith(\".meta.json\"):\n",
    "            meta_path = os.path.join(input_datasets_directory, file)\n",
    "\n",
    "            with open(meta_path) as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            corr_dict = metadata.get(\"feature_target_correlations\", {})\n",
    "            if corr_dict:\n",
    "                top_corr_feature = max(corr_dict.items(), key=lambda x: abs(x[1]))\n",
    "                top_corr_name = top_corr_feature[0]\n",
    "                top_corr_value = top_corr_feature[1]\n",
    "            else:\n",
    "                top_corr_name = None\n",
    "                top_corr_value = None\n",
    "\n",
    "            summary.append({\n",
    "                \"Dataset Name\": metadata.get(\"dataset_name\"),\n",
    "                \"Source\": metadata.get(\"source\", \"OpenML\"),\n",
    "                \"ID/Code\": metadata.get(\"code\") or metadata.get(\"id\"),\n",
    "                \"Num Observations\": metadata.get(\"num_observations\"),\n",
    "                \"Num Features\": metadata.get(\"num_features\"),\n",
    "                \"Categorical Features\": len(metadata.get(\"categorical_features\", [])),\n",
    "                \"Numeric Features\": len(metadata.get(\"numeric_features\", [])),\n",
    "                \"% Null Values\": metadata.get(\"%_null_values\"),\n",
    "                \"Target\": metadata.get(\"target\"),\n",
    "                \"Target Mean\": metadata.get(\"target_mean\"),\n",
    "                \"Target Median\": metadata.get(\"target_median\"),\n",
    "                \"Target Min\": metadata.get(\"target_min\"),\n",
    "                \"Target Max\": metadata.get(\"target_max\"),\n",
    "                \"Target Std Dev\": metadata.get(\"target_std_dev\"),\n",
    "                \"Target IQR\": metadata.get(\"target_IQR\"),\n",
    "                \"Target Skewness\": metadata.get(\"target_skewness\"),\n",
    "                \"Target Outlier %\": metadata.get(\"target_outlier_%\"),\n",
    "                \"Target CV\": metadata.get(\"target_coefficient_of_variation\"),\n",
    "                \"Target Unique Values\": metadata.get(\"target_unique_values\"),\n",
    "                \"Top Correlated Feature\": top_corr_name,\n",
    "                \"Top Correlation Value\": top_corr_value,\n",
    "                \"Domain\": metadata.get(\"domain\", \"unknown\")\n",
    "            })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    df_summary.sort_values(by=[\"Num Observations\", \"Num Features\"], inplace=True)\n",
    "\n",
    "    output_path = os.path.join(output_table_directory, \"datasets_summary_table.csv\")\n",
    "    df_summary.to_csv(output_path, index=False)\n",
    "    print(f\"📊 Tabela zbiorcza zapisana w: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_regression(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocessing dataset dla problemu regresji.\n",
    "    Wykonywane kroki:\n",
    "      1. Usunięcie duplikatów.\n",
    "      2. Usunięcie kolumn (poza targetem) zawierających tylko jedną unikalną wartość.\n",
    "      3. Imputacja brakujących wartości:\n",
    "         - cechy numeryczne: średnia,\n",
    "         - cechy kategoryczne: wartość 'Missing'.\n",
    "      4. One-hot encoding cech kategorycznych.\n",
    "      5. Minmax wszystkich cech (po transformacjach).\n",
    "    \"\"\"\n",
    "    # Krok 1: Usunięcie duplikatów\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Krok 2: Usunięcie kolumn z jedną unikalną wartością (pomijamy target)\n",
    "    cols_to_drop = [col for col in df.columns if col != target_col and df[col].nunique() == 1]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Oddzielenie cech od zmiennej docelowej\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # Rozpoznanie kolumn numerycznych i kategorycznych\n",
    "    num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Budowa transformera:\n",
    "    # - Cechy numeryczne: imputacja średnią.\n",
    "    # - Cechy kategoryczne: imputacja braków stałą wartością 'Missing' + one-hot encoding.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='mean'), num_cols),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "                ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), cat_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Pipeline: najpierw transformacja, i minmax scaler\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    # Dopasowanie i transformacja cech\n",
    "    X_processed = pipeline.fit_transform(X)\n",
    "    \n",
    "    # Odtworzenie nazw kolumn\n",
    "    new_columns = []\n",
    "    new_columns.extend(num_cols)\n",
    "    if cat_cols:\n",
    "        onehot = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "        new_cat_cols = onehot.get_feature_names_out(cat_cols)\n",
    "        new_columns.extend(new_cat_cols)\n",
    "    \n",
    "    X_processed = pd.DataFrame(X_processed, columns=new_columns, index=X.index)\n",
    "    \n",
    "    # Połączenie przetworzonych cech z oryginalną kolumną target\n",
    "    df_processed = X_processed.copy()\n",
    "    df_processed[target_col] = y\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Funkcja przechodzi przez wszystkie pliki CSV w katalogu input_path.\n",
    "    Dla każdego pliku:\n",
    "      - Odczytuje odpowiadający plik metadanych (.meta.json) i wyciąga nazwę kolumny target.\n",
    "      - Wczytuje dane z CSV.\n",
    "      - Przetwarza dane za pomocą funkcji preprocess_dataset_regression.\n",
    "      - Zapisuje wynikowy DataFrame jako CSV do katalogu output_path przy zachowaniu oryginalnej nazwy pliku.\n",
    "      - Kopiuje również plik metadanych (.meta.json) do katalogu wyjściowego.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(input_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(input_path, file)\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            meta_filename = base_name + \".meta.json\"\n",
    "            meta_path = os.path.join(input_path, meta_filename)\n",
    "            \n",
    "            if not os.path.exists(meta_path):\n",
    "                print(f\"Brak pliku metadanych dla {file}. Pomijam plik.\")\n",
    "                continue\n",
    "            \n",
    "            # Wczytanie metadanych\n",
    "            with open(meta_path, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            target_col = metadata.get(\"target\")\n",
    "            if not target_col:\n",
    "                print(f\"Nie znaleziono nazwy kolumny target w metadanych dla {file}. Pomijam plik.\")\n",
    "                continue\n",
    "            \n",
    "            # Wczytanie danych\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się wczytać pliku {csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Przetwarzanie danych\n",
    "            try:\n",
    "                df_processed = preprocess_dataset_regression(df, target_col)\n",
    "            except Exception as e:\n",
    "                print(f\"Błąd przy przetwarzaniu {csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Zapis przetworzonego CSV\n",
    "            output_csv_path = os.path.join(output_path, file)\n",
    "            try:\n",
    "                df_processed.to_csv(output_csv_path, index=False)\n",
    "                print(f\"✔ Przetworzono i zapisano: {output_csv_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się zapisać pliku {output_csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Kopiowanie pliku metadanych do katalogu wyjściowego\n",
    "            output_meta_path = os.path.join(output_path, meta_filename)\n",
    "            try:\n",
    "                shutil.copy(meta_path, output_meta_path)\n",
    "                print(f\"📁 Skopiowano metadane: {output_meta_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się skopiować metadanych dla {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ids = [\n",
    "    189,\n",
    "    925,\n",
    "    186,\n",
    "    320,\n",
    "    1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Zapisano ./uci_datasets\\Parkinsons_Telemonitoring_189.csv\n",
      "✔ Zapisano ./uci_datasets\\Infrared_Thermography_Temperature_925.csv\n",
      "✔ Zapisano ./uci_datasets\\Wine_Quality_186.csv\n",
      "✔ Zapisano ./uci_datasets\\Student_Performance_320.csv\n",
      "✔ Zapisano ./uci_datasets\\Abalone_1.csv\n",
      "📊 Tabela zbiorcza zapisana w: ./datasets_summary\\datasets_summary_table.csv\n"
     ]
    }
   ],
   "source": [
    "download_uci_datasets(\n",
    "    dataset_ids=datasets_ids,\n",
    "    directory_path=\"./uci_datasets\"\n",
    ")\n",
    "generate_dataset_summary_table(\n",
    "    input_datasets_directory=\"./uci_datasets\",\n",
    "    output_table_directory=\"./datasets_summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ID/Code</th>\n",
       "      <th>Num Observations</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Categorical Features</th>\n",
       "      <th>Numeric Features</th>\n",
       "      <th>% Null Values</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Target Max</th>\n",
       "      <th>Target Std Dev</th>\n",
       "      <th>Target IQR</th>\n",
       "      <th>Target Skewness</th>\n",
       "      <th>Target Outlier %</th>\n",
       "      <th>Target CV</th>\n",
       "      <th>Target Unique Values</th>\n",
       "      <th>Top Correlated Feature</th>\n",
       "      <th>Top Correlation Value</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student Performance</td>\n",
       "      <td>UCI</td>\n",
       "      <td>320</td>\n",
       "      <td>649</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>G3</td>\n",
       "      <td>11.906</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000</td>\n",
       "      <td>3.231</td>\n",
       "      <td>4.000</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.271</td>\n",
       "      <td>17</td>\n",
       "      <td>failures</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Infrared Thermography Temperature</td>\n",
       "      <td>UCI</td>\n",
       "      <td>925</td>\n",
       "      <td>1020</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>aveOralF</td>\n",
       "      <td>36.979</td>\n",
       "      <td>...</td>\n",
       "      <td>39.600</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.537</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.010</td>\n",
       "      <td>53</td>\n",
       "      <td>T_Max1</td>\n",
       "      <td>0.753</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>UCI</td>\n",
       "      <td>1</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Rings</td>\n",
       "      <td>9.934</td>\n",
       "      <td>...</td>\n",
       "      <td>29.000</td>\n",
       "      <td>3.224</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.325</td>\n",
       "      <td>28</td>\n",
       "      <td>Shell_weight</td>\n",
       "      <td>0.628</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parkinsons Telemonitoring</td>\n",
       "      <td>UCI</td>\n",
       "      <td>189</td>\n",
       "      <td>5875</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>motor_UPDRS</td>\n",
       "      <td>21.296</td>\n",
       "      <td>...</td>\n",
       "      <td>39.511</td>\n",
       "      <td>8.129</td>\n",
       "      <td>12.596</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.382</td>\n",
       "      <td>1080</td>\n",
       "      <td>age</td>\n",
       "      <td>0.274</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wine Quality</td>\n",
       "      <td>UCI</td>\n",
       "      <td>186</td>\n",
       "      <td>6497</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>quality</td>\n",
       "      <td>5.818</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.150</td>\n",
       "      <td>7</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>0.444</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Dataset Name Source  ID/Code  Num Observations  \\\n",
       "0                Student Performance    UCI      320               649   \n",
       "1  Infrared Thermography Temperature    UCI      925              1020   \n",
       "2                            Abalone    UCI        1              4177   \n",
       "3          Parkinsons Telemonitoring    UCI      189              5875   \n",
       "4                       Wine Quality    UCI      186              6497   \n",
       "\n",
       "   Num Features  Categorical Features  Numeric Features  % Null Values  \\\n",
       "0            30                    17                13           0.00   \n",
       "1            33                     3                30           0.01   \n",
       "2             8                     1                 7           0.00   \n",
       "3            19                     0                19           0.00   \n",
       "4            11                     0                11           0.00   \n",
       "\n",
       "        Target  Target Mean  ...  Target Max  Target Std Dev  Target IQR  \\\n",
       "0           G3       11.906  ...      19.000           3.231       4.000   \n",
       "1     aveOralF       36.979  ...      39.600           0.386       0.300   \n",
       "2        Rings        9.934  ...      29.000           3.224       3.000   \n",
       "3  motor_UPDRS       21.296  ...      39.511           8.129      12.596   \n",
       "4      quality        5.818  ...       9.000           0.873       1.000   \n",
       "\n",
       "   Target Skewness  Target Outlier %  Target CV  Target Unique Values  \\\n",
       "0           -0.911              2.47      0.271                    17   \n",
       "1            2.537              2.84      0.010                    53   \n",
       "2            1.114              1.48      0.325                    28   \n",
       "3            0.075              0.00      0.382                  1080   \n",
       "4            0.190              0.54      0.150                     7   \n",
       "\n",
       "   Top Correlated Feature  Top Correlation Value   Domain  \n",
       "0                failures                 -0.393  unknown  \n",
       "1                  T_Max1                  0.753  unknown  \n",
       "2            Shell_weight                  0.628  unknown  \n",
       "3                     age                  0.274  unknown  \n",
       "4                 alcohol                  0.444  unknown  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = pd.read_csv(\"datasets_summary/datasets_summary_table.csv\")\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Przetworzono i zapisano: ./uci_datasets_preprocessed\\Abalone_1.csv\n",
      "📁 Skopiowano metadane: ./uci_datasets_preprocessed\\Abalone_1.meta.json\n",
      "✔ Przetworzono i zapisano: ./uci_datasets_preprocessed\\Infrared_Thermography_Temperature_925.csv\n",
      "📁 Skopiowano metadane: ./uci_datasets_preprocessed\\Infrared_Thermography_Temperature_925.meta.json\n",
      "✔ Przetworzono i zapisano: ./uci_datasets_preprocessed\\Parkinsons_Telemonitoring_189.csv\n",
      "📁 Skopiowano metadane: ./uci_datasets_preprocessed\\Parkinsons_Telemonitoring_189.meta.json\n",
      "✔ Przetworzono i zapisano: ./uci_datasets_preprocessed\\Student_Performance_320.csv\n",
      "📁 Skopiowano metadane: ./uci_datasets_preprocessed\\Student_Performance_320.meta.json\n",
      "✔ Przetworzono i zapisano: ./uci_datasets_preprocessed\\Wine_Quality_186.csv\n",
      "📁 Skopiowano metadane: ./uci_datasets_preprocessed\\Wine_Quality_186.meta.json\n"
     ]
    }
   ],
   "source": [
    "preprocess_datasets(\n",
    "    input_path=\"./uci_datasets\",\n",
    "    output_path=\"./uci_datasets_preprocessed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./uci_datasets_preprocessed/Parkinsons_Telemonitoring_189.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>test_time</th>\n",
       "      <th>Jitter(%)</th>\n",
       "      <th>Jitter(Abs)</th>\n",
       "      <th>Jitter:RAP</th>\n",
       "      <th>Jitter:PPQ5</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>Shimmer</th>\n",
       "      <th>Shimmer(dB)</th>\n",
       "      <th>Shimmer:APQ3</th>\n",
       "      <th>Shimmer:APQ5</th>\n",
       "      <th>Shimmer:APQ11</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>PPE</th>\n",
       "      <th>sex</th>\n",
       "      <th>motor_UPDRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.045076</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>0.039635</td>\n",
       "      <td>0.064433</td>\n",
       "      <td>0.085062</td>\n",
       "      <td>0.098030</td>\n",
       "      <td>0.079287</td>\n",
       "      <td>0.067543</td>\n",
       "      <td>0.051764</td>\n",
       "      <td>0.079267</td>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.551717</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>0.097793</td>\n",
       "      <td>0.194544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.077034</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.032819</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.064691</td>\n",
       "      <td>0.073522</td>\n",
       "      <td>0.051720</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.052753</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.014474</td>\n",
       "      <td>0.704771</td>\n",
       "      <td>0.348330</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.121335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.108957</td>\n",
       "      <td>0.040137</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.023868</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>0.035577</td>\n",
       "      <td>0.039375</td>\n",
       "      <td>0.044291</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.026651</td>\n",
       "      <td>0.590568</td>\n",
       "      <td>0.381812</td>\n",
       "      <td>0.085362</td>\n",
       "      <td>0.265104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.136105</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.054924</td>\n",
       "      <td>0.027618</td>\n",
       "      <td>0.031969</td>\n",
       "      <td>0.027673</td>\n",
       "      <td>0.075423</td>\n",
       "      <td>0.144642</td>\n",
       "      <td>0.058674</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.062791</td>\n",
       "      <td>0.058632</td>\n",
       "      <td>0.036834</td>\n",
       "      <td>0.629169</td>\n",
       "      <td>0.412583</td>\n",
       "      <td>0.181761</td>\n",
       "      <td>0.437884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.172487</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>0.040263</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.052604</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.044524</td>\n",
       "      <td>0.057515</td>\n",
       "      <td>0.032121</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.675585</td>\n",
       "      <td>0.393664</td>\n",
       "      <td>0.134202</td>\n",
       "      <td>0.241814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  test_time  Jitter(%)  Jitter(Abs)  Jitter:RAP  Jitter:PPQ5  \\\n",
       "0  0.734694   0.045076   0.058390     0.071164    0.064324     0.039635   \n",
       "1  0.734694   0.077034   0.021884     0.032819    0.017305     0.015478   \n",
       "2  0.734694   0.108957   0.040137     0.050413    0.030065     0.023868   \n",
       "3  0.734694   0.136105   0.044877     0.054924    0.027618     0.031969   \n",
       "4  0.734694   0.172487   0.025413     0.040263    0.010488     0.012585   \n",
       "\n",
       "   Jitter:DDP   Shimmer  Shimmer(dB)  Shimmer:APQ3  Shimmer:APQ5  \\\n",
       "0    0.064433  0.085062     0.098030      0.079287      0.067543   \n",
       "1    0.017303  0.064691     0.073522      0.051720      0.053186   \n",
       "2    0.030178  0.051549     0.074483      0.035577      0.039375   \n",
       "3    0.027673  0.075423     0.144642      0.058674      0.064878   \n",
       "4    0.010486  0.052604     0.072081      0.032162      0.044524   \n",
       "\n",
       "   Shimmer:APQ11  Shimmer:DDA       NHR       HNR      RPDE       DFA  \\\n",
       "0       0.051764     0.079267  0.018723  0.551717  0.328638  0.097793   \n",
       "1       0.052753     0.051699  0.014474  0.704771  0.348330  0.144300   \n",
       "2       0.044291     0.035556  0.026651  0.590568  0.381812  0.085362   \n",
       "3       0.062791     0.058632  0.036834  0.629169  0.412583  0.181761   \n",
       "4       0.057515     0.032121  0.015160  0.675585  0.393664  0.134202   \n",
       "\n",
       "        PPE  sex  motor_UPDRS  \n",
       "0  0.194544  0.0       28.199  \n",
       "1  0.121335  0.0       28.447  \n",
       "2  0.265104  0.0       28.695  \n",
       "3  0.437884  0.0       28.905  \n",
       "4  0.241814  0.0       29.187  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
